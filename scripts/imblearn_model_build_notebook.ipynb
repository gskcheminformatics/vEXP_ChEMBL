{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sweet-starter",
   "metadata": {},
   "source": [
    "# Model Building Setup for Safety Models using RdKit descriptors and ChEMBL31 data only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-niagara",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "911ceaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "model_dir = \"AHR_PEC50\"\n",
    "data_file_name = \"./AHR_PEC50_train.tsv\"\n",
    "# output\n",
    "scalar_file_name = \"./scalar_AHR_PEC50.pkl\"\n",
    "ecc_model_file = \"./AHR_PEC50_ecc.pkl\"\n",
    "brf_model_file = \"./AHR_PEC50_brf.pkl\"\n",
    "rusb_model_file = \"./AHR_PEC50_rusb.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "italian-sodium",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, balanced_accuracy_score, matthews_corrcoef, cohen_kappa_score \n",
    "from utils import rdkit_fpconvert_numpy, rdkit_get_physchem_descr\n",
    "\n",
    "seed = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "after-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.ensemble import EasyEnsembleClassifier,BalancedRandomForestClassifier,RUSBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-madrid",
   "metadata": {},
   "source": [
    "# Get Mols, Data and generate descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dramatic-wrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_file_name, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "gothic-milwaukee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_str = df.class_label.tolist() \n",
    "y_int = pd.get_dummies(y_str)\n",
    "y_new = y_int[\"POSITIVE\"] \n",
    "df['class_label_binary'] = y_new \n",
    "y = df.class_label_binary.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "warming-referral",
   "metadata": {},
   "outputs": [],
   "source": [
    "mols = [Chem.MolFromSmiles(smi) for smi in df.parentised_smiles]\n",
    "# generate binary Morgan fingerprint with radius 2\n",
    "fp = [AllChem.GetMorganFingerprintAsBitVect(m, 2) for m in mols]\n",
    "# generate binary Morgan fingerprint with radius 2 with physchem as numpy array\n",
    "x = rdkit_fpconvert_numpy(fp)\n",
    "x = np.concatenate((x, rdkit_get_physchem_descr(mols)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-blogger",
   "metadata": {},
   "source": [
    "## Scale using scalar from last run and calculate splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "major-lingerie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select 20% of compounds as test set but with stratified selection\n",
    "x_tr, x_ts, y_tr, y_ts = train_test_split(x, y, test_size=0.20, random_state=seed, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "compact-structure",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/mydata/stv/jal44287/CONDA/qsar_imb/lib/python3.12/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.0.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# save scalar\n",
    "scale = joblib.load(scalar_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "seventh-folder",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr = scale.transform(x_tr)\n",
    "x_ts = scale.transform(x_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ac5546",
   "metadata": {},
   "source": [
    "# IMBLearn Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee9d7d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV] END ....................................n_estimators=10; total time=   1.8s\n",
      "[CV] END ....................................n_estimators=10; total time=   2.0s\n",
      "[CV] END ....................................n_estimators=10; total time=   2.0s\n",
      "[CV] END ....................................n_estimators=10; total time=   2.1s\n",
      "[CV] END ....................................n_estimators=10; total time=   2.1s\n",
      "[CV] END ....................................n_estimators=30; total time=   6.2s\n",
      "[CV] END ....................................n_estimators=30; total time=   6.3s\n",
      "[CV] END ....................................n_estimators=30; total time=   6.3s\n",
      "[CV] END ....................................n_estimators=30; total time=   6.1s\n",
      "[CV] END ....................................n_estimators=30; total time=   6.0s\n",
      "[CV] END ....................................n_estimators=50; total time=  10.6s\n",
      "[CV] END ....................................n_estimators=50; total time=   9.8s\n",
      "[CV] END ....................................n_estimators=50; total time=  10.1s\n",
      "[CV] END ....................................n_estimators=50; total time=   8.0s\n",
      "[CV] END ....................................n_estimators=50; total time=  10.2s\n",
      "[CV] END ...................................n_estimators=100; total time=  25.4s\n",
      "[CV] END ...................................n_estimators=100; total time=  19.9s\n",
      "[CV] END ...................................n_estimators=100; total time=  20.0s\n",
      "[CV] END ...................................n_estimators=100; total time=  20.1s\n",
      "[CV] END ...................................n_estimators=100; total time=  20.2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./AHR_PEC50_ecc.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create grid search dictionary\n",
    "eec_param_grid = {\"n_estimators\": [i for i in [10, 30, 50, 100]]}\n",
    "# model build\n",
    "eec = GridSearchCV(EasyEnsembleClassifier(n_jobs=-1),\n",
    "                  verbose=2,\n",
    "                  scoring='balanced_accuracy',\n",
    "                  param_grid=eec_param_grid, cv=5,\n",
    "                  n_jobs=-1)\n",
    "eec.fit(x_tr, y_tr)\n",
    "# save model\n",
    "joblib.dump(eec, ecc_model_file, compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "similar-philip",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV] END ....................................n_estimators=10; total time=   0.0s\n",
      "[CV] END ....................................n_estimators=10; total time=   0.0s\n",
      "[CV] END ....................................n_estimators=10; total time=   0.0s\n",
      "[CV] END ....................................n_estimators=10; total time=   0.0s\n",
      "[CV] END ....................................n_estimators=10; total time=   0.0s\n",
      "[CV] END ....................................n_estimators=30; total time=   0.1s\n",
      "[CV] END ....................................n_estimators=30; total time=   0.1s\n",
      "[CV] END ....................................n_estimators=30; total time=   0.1s\n",
      "[CV] END ....................................n_estimators=30; total time=   0.1s\n",
      "[CV] END ....................................n_estimators=30; total time=   0.1s\n",
      "[CV] END ....................................n_estimators=50; total time=   0.2s\n",
      "[CV] END ....................................n_estimators=50; total time=   0.2s\n",
      "[CV] END ....................................n_estimators=50; total time=   0.2s\n",
      "[CV] END ....................................n_estimators=50; total time=   0.2s\n",
      "[CV] END ....................................n_estimators=50; total time=   0.2s\n",
      "[CV] END ...................................n_estimators=100; total time=   0.4s\n",
      "[CV] END ...................................n_estimators=100; total time=   0.3s\n",
      "[CV] END ...................................n_estimators=100; total time=   0.3s\n",
      "[CV] END ...................................n_estimators=100; total time=   0.3s\n",
      "[CV] END ...................................n_estimators=100; total time=   0.3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./AHR_PEC50_brf.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create grid search dictionary\n",
    "brf_param_grid = {\"n_estimators\": [i for i in [10, 30, 50, 100]]}\n",
    "# model build\n",
    "brf = GridSearchCV(BalancedRandomForestClassifier(n_jobs=-1, replacement=True, sampling_strategy='all'),\n",
    "                  verbose=2,\n",
    "                  scoring='balanced_accuracy',\n",
    "                  param_grid=eec_param_grid, cv=5,\n",
    "                  n_jobs=-1)\n",
    "brf.fit(x_tr, y_tr)\n",
    "# save model\n",
    "joblib.dump(brf, brf_model_file, compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "postal-minute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV] END ....................................n_estimators=10; total time=   0.1s\n",
      "[CV] END ....................................n_estimators=10; total time=   0.1s\n",
      "[CV] END ....................................n_estimators=10; total time=   0.1s\n",
      "[CV] END ....................................n_estimators=10; total time=   0.1s\n",
      "[CV] END ....................................n_estimators=10; total time=   0.1s\n",
      "[CV] END ....................................n_estimators=30; total time=   0.2s\n",
      "[CV] END ....................................n_estimators=30; total time=   0.1s\n",
      "[CV] END ....................................n_estimators=30; total time=   0.2s\n",
      "[CV] END ....................................n_estimators=30; total time=   0.2s\n",
      "[CV] END ....................................n_estimators=30; total time=   0.2s\n",
      "[CV] END ....................................n_estimators=50; total time=   0.3s\n",
      "[CV] END ....................................n_estimators=50; total time=   0.3s\n",
      "[CV] END ....................................n_estimators=50; total time=   0.3s\n",
      "[CV] END ....................................n_estimators=50; total time=   0.3s\n",
      "[CV] END ....................................n_estimators=50; total time=   0.3s\n",
      "[CV] END ...................................n_estimators=100; total time=   0.5s\n",
      "[CV] END ...................................n_estimators=100; total time=   0.5s\n",
      "[CV] END ...................................n_estimators=100; total time=   0.6s\n",
      "[CV] END ...................................n_estimators=100; total time=   0.5s\n",
      "[CV] END ...................................n_estimators=100; total time=   0.5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./AHR_PEC50_rusb.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create grid search dictionary\n",
    "rusb_param_grid = {\"n_estimators\": [i for i in [10, 30, 50, 100]]}\n",
    "# model build\n",
    "rusb = GridSearchCV(RUSBoostClassifier(),\n",
    "                  verbose=2,\n",
    "                  scoring='balanced_accuracy',\n",
    "                  param_grid=eec_param_grid, cv=5,\n",
    "                  n_jobs=-1)\n",
    "rusb.fit(x_tr, y_tr)\n",
    "# save model\n",
    "joblib.dump(eec, rusb_model_file, compress=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638fe83a",
   "metadata": {},
   "source": [
    "## Test Set Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "expressed-monday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEC Train:\n",
      "Precision =  1.0\n",
      "ROC-AUC =  0.9663461538461539\n",
      "Balanced Accuracy Score =  0.9663461538461539\n",
      "MCC =  0.9162003475894762\n",
      "Kappa =  0.9127025614218505\n"
     ]
    }
   ],
   "source": [
    "# predict for the test set compounds\n",
    "pred_eec_tr = eec.predict(x_tr)\n",
    "# calc statistics\n",
    "print(\"EEC Train:\")\n",
    "print(\"Precision = \", precision_score(y_tr, pred_eec_tr))\n",
    "print(\"ROC-AUC = \", roc_auc_score(y_tr, pred_eec_tr))\n",
    "print(\"Balanced Accuracy Score = \", balanced_accuracy_score(y_tr, pred_eec_tr))\n",
    "print(\"MCC = \", matthews_corrcoef(y_tr, pred_eec_tr))\n",
    "print(\"Kappa = \", cohen_kappa_score(y_tr, pred_eec_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e5f09ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEC Test:\n",
      "Precision =  0.72\n",
      "ROC-AUC =  0.6274038461538461\n",
      "Balanced Accuracy Score =  0.6274038461538461\n",
      "MCC =  0.2520952919018819\n",
      "Kappa =  0.25178147268408546\n"
     ]
    }
   ],
   "source": [
    "# predict for the test set compounds\n",
    "pred_eec_ts = eec.predict(x_ts)\n",
    "# calc statistics\n",
    "print(\"EEC Test:\")\n",
    "print(\"Precision = \", precision_score(y_ts, pred_eec_ts))\n",
    "print(\"ROC-AUC = \", roc_auc_score(y_ts, pred_eec_ts))\n",
    "print(\"Balanced Accuracy Score = \", balanced_accuracy_score(y_ts, pred_eec_ts))\n",
    "print(\"MCC = \", matthews_corrcoef(y_ts, pred_eec_ts))\n",
    "print(\"Kappa = \", cohen_kappa_score(y_ts, pred_eec_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "maritime-indie",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRF Train:\n",
      "Precision =  0.9897959183673469\n",
      "ROC-AUC =  0.958409645909646\n",
      "Balanced Accuracy Score =  0.9584096459096458\n",
      "MCC =  0.902470686576531\n",
      "Kappa =  0.8999250936329588\n"
     ]
    }
   ],
   "source": [
    "# calc statistics\n",
    "# predict for the test set compounds\n",
    "pred_brf_tr = brf.predict(x_tr)\n",
    "# calc statistics\n",
    "print(\"BRF Train:\")\n",
    "print(\"Precision = \", precision_score(y_tr, pred_brf_tr))\n",
    "print(\"ROC-AUC = \", roc_auc_score(y_tr, pred_brf_tr))\n",
    "print(\"Balanced Accuracy Score = \", balanced_accuracy_score(y_tr, pred_brf_tr))\n",
    "print(\"MCC = \", matthews_corrcoef(y_tr, pred_brf_tr))\n",
    "print(\"Kappa = \", cohen_kappa_score(y_tr, pred_brf_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "latin-immigration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRF Train:\n",
      "Precision =  0.75\n",
      "ROC-AUC =  0.6850961538461539\n",
      "Balanced Accuracy Score =  0.6850961538461539\n",
      "MCC =  0.381356384904845\n",
      "Kappa =  0.3793103448275862\n"
     ]
    }
   ],
   "source": [
    "# calc statistics\n",
    "# predict for the test set compounds\n",
    "pred_brf_ts = brf.predict(x_ts)\n",
    "# calc statistics\n",
    "print(\"BRF Train:\")\n",
    "print(\"Precision = \", precision_score(y_ts, pred_brf_ts))\n",
    "print(\"ROC-AUC = \", roc_auc_score(y_ts, pred_brf_ts))\n",
    "print(\"Balanced Accuracy Score = \", balanced_accuracy_score(y_ts, pred_brf_ts))\n",
    "print(\"MCC = \", matthews_corrcoef(y_ts, pred_brf_ts))\n",
    "print(\"Kappa = \", cohen_kappa_score(y_ts, pred_brf_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "exact-involvement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUSB Train:\n",
      "Precision =  1.0\n",
      "ROC-AUC =  0.9951923076923077\n",
      "Balanced Accuracy Score =  0.9951923076923077\n",
      "MCC =  0.9873752355458539\n",
      "Kappa =  0.9872955496386459\n"
     ]
    }
   ],
   "source": [
    "# calc statistics\n",
    "# predict for the test set compounds\n",
    "pred_rusb_tr = rusb.predict(x_tr)\n",
    "# calc statistics\n",
    "print(\"RUSB Train:\")\n",
    "print(\"Precision = \", precision_score(y_tr, pred_rusb_tr))\n",
    "print(\"ROC-AUC = \", roc_auc_score(y_tr, pred_rusb_tr))\n",
    "print(\"Balanced Accuracy Score = \", balanced_accuracy_score(y_tr, pred_rusb_tr))\n",
    "print(\"MCC = \", matthews_corrcoef(y_tr, pred_rusb_tr))\n",
    "print(\"Kappa = \", cohen_kappa_score(y_tr, pred_rusb_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "formal-chinese",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUSB Train:\n",
      "Precision =  0.7083333333333334\n",
      "ROC-AUC =  0.6081730769230769\n",
      "Balanced Accuracy Score =  0.6081730769230769\n",
      "MCC =  0.2123019439021117\n",
      "Kappa =  0.21126760563380276\n"
     ]
    }
   ],
   "source": [
    "# calc statistics\n",
    "# predict for the test set compounds\n",
    "pred_rusb_ts = rusb.predict(x_ts)\n",
    "# calc statistics\n",
    "print(\"RUSB Train:\")\n",
    "print(\"Precision = \", precision_score(y_ts, pred_rusb_ts))\n",
    "print(\"ROC-AUC = \", roc_auc_score(y_ts, pred_rusb_ts))\n",
    "print(\"Balanced Accuracy Score = \", balanced_accuracy_score(y_ts, pred_rusb_ts))\n",
    "print(\"MCC = \", matthews_corrcoef(y_ts, pred_rusb_ts))\n",
    "print(\"Kappa = \", cohen_kappa_score(y_ts, pred_rusb_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-ocean",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-westminster",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-customer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-bradley",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_qsar_imb)",
   "language": "python",
   "name": "conda_qsar_imb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "b2bb8ba4dbbd1922056eae260ebdf0a3570f5d6445cea652291a72b5d24d4404"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
